{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb0e756-580c-4113-b84c-580dd5622b60",
   "metadata": {},
   "source": [
    "## Data Ingestion and Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4d9952-8561-43aa-bea8-0f9e8aed01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ccd9a9-9b33-4ef4-b90d-82871cee3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the review dataset\n",
    "review_path = \"yelp_academic_dataset_review.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb9ba6e-0c12-46af-9a44-9be2e550aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the review dataset without excessive logs\n",
    "def load_review_dataset(file_path, chunk_size=10000):\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    data = []\n",
    "    \n",
    "    # Read the JSON file line by line\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Completed loading {file_path}. Total lines: {len(data)}\")\n",
    "    return data  # Return the dataset as a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8a2229-f59b-47a1-a527-9d33d8513628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yelp_academic_dataset_review.json...\n",
      "Completed loading yelp_academic_dataset_review.json. Total lines: 6990280\n",
      "First record in the review dataset:\n",
      "{'review_id': 'KU_O5udG6zpxOg-VcAEodg', 'user_id': 'mh_-eMZ6K5RLWhZyISBhwA', 'business_id': 'XQfwVwDr-v0ZS3_CbbE5Xw', 'stars': 3.0, 'useful': 0, 'funny': 0, 'cool': 0, 'text': \"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\", 'date': '2018-07-07 22:09:11'}\n"
     ]
    }
   ],
   "source": [
    "# Load the review dataset\n",
    "review_data = load_review_dataset(review_path)\n",
    "\n",
    "# Display the first record to verify\n",
    "print(\"First record in the review dataset:\")\n",
    "print(review_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34dcbb93-4848-4523-acdd-911b993f5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered reviews: 6990280 records\n",
      "First filtered record:\n",
      "{'text': \"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\", 'stars': 3.0, 'cool': 0, 'funny': 0, 'useful': 0}\n"
     ]
    }
   ],
   "source": [
    "# Filter out relevant columns: text, stars, cool, funny, useful\n",
    "filtered_reviews = [\n",
    "    {\n",
    "        \"text\": record[\"text\"],\n",
    "        \"stars\": record[\"stars\"],\n",
    "        \"cool\": record[\"cool\"],\n",
    "        \"funny\": record[\"funny\"],\n",
    "        \"useful\": record[\"useful\"]\n",
    "    }\n",
    "    for record in review_data\n",
    "]\n",
    "\n",
    "# Display the first filtered record\n",
    "print(f\"Filtered reviews: {len(filtered_reviews)} records\")\n",
    "print(\"First filtered record:\")\n",
    "print(filtered_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d519f3d2-3d6f-42ec-b2b3-8107f71f0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First record with sentiment:\n",
      "{'text': \"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\", 'stars': 3.0, 'cool': 0, 'funny': 0, 'useful': 0, 'sentiment': 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "# Function to label sentiments\n",
    "def label_sentiment(stars):\n",
    "    if stars >= 4:\n",
    "        return \"Positive\"\n",
    "    elif stars == 3:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "\n",
    "# Add sentiment labels\n",
    "for review in filtered_reviews:\n",
    "    review[\"sentiment\"] = label_sentiment(review[\"stars\"])\n",
    "\n",
    "# Display the first record with sentiment\n",
    "print(\"First record with sentiment:\")\n",
    "print(filtered_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e4c47-0761-4cc6-b8c3-b87b7a7b713c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cafd945-cd64-4f6e-95b7-4b35177e7afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained tokenizer (BERT tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab303dd5-4a15-406f-8233-21459cd32213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_tokenizer(batch):\n",
    "    # Tokenize the batch of text on the GPU\n",
    "    tokenized = tokenizer(\n",
    "        batch,\n",
    "        padding=True,          # Pad sentences to the same length\n",
    "        truncation=True,       # Truncate long sentences\n",
    "        max_length=128,        # Limit sequence length to 128 tokens\n",
    "        return_tensors=\"pt\"    # Return PyTorch tensors\n",
    "    ).to(\"cuda\")              # Move the result to the GPU\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7543a959-dced-4816-b858-0f0b05f82dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size: 1000000 reviews\n",
      "First review in the subset:\n",
      "{'text': 'Hibachi Station was delicious and hot and fresh clean restaurant.. stacked bar... Clean restroom', 'stars': 3.0, 'cool': 0, 'funny': 0, 'useful': 0, 'sentiment': 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define subset size\n",
    "subset_size = 1000000\n",
    "\n",
    "# Randomly sample a subset of the data\n",
    "subset_reviews = random.sample(filtered_reviews, subset_size)\n",
    "\n",
    "# Display the size of the subset\n",
    "print(f\"Subset size: {len(subset_reviews)} reviews\")\n",
    "print(\"First review in the subset:\")\n",
    "print(subset_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b2da96a-a2d3-4f14-a6f4-3fcf2a540824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 out of 11...\n",
      "Processing batch 2 out of 11...\n",
      "Processing batch 3 out of 11...\n",
      "Processing batch 4 out of 11...\n",
      "Processing batch 5 out of 11...\n",
      "Processing batch 6 out of 11...\n",
      "Processing batch 7 out of 11...\n",
      "Processing batch 8 out of 11...\n",
      "Processing batch 9 out of 11...\n",
      "Processing batch 10 out of 11...\n",
      "First record after GPU-accelerated tokenization:\n",
      "{'text': 'Hibachi Station was delicious and hot and fresh clean restaurant.. stacked bar... Clean restroom', 'stars': 3.0, 'cool': 0, 'funny': 0, 'useful': 0, 'sentiment': 'Neutral', 'processed_text': [101, 7632, 7693, 2072, 2276, 2001, 12090, 1998, 2980, 1998, 4840, 4550, 4825, 1012, 1012, 16934, 3347, 1012, 1012, 1012, 4550, 28249, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "batch_size = 100000\n",
    "\n",
    "# Extract text data from the subset\n",
    "texts = [review[\"text\"] for review in subset_reviews]\n",
    "\n",
    "# Process the text in batches\n",
    "processed_batches = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    print(f\"Processing batch {i // batch_size + 1} out of {len(texts) // batch_size + 1}...\")\n",
    "    batch = texts[i:i + batch_size]\n",
    "    tokenized_batch = preprocess_text_with_tokenizer(batch)  # Tokenize with GPU acceleration\n",
    "    processed_batches.append(tokenized_batch[\"input_ids\"])  # Only store input IDs\n",
    "\n",
    "# Flatten the processed batches into a single tensor\n",
    "processed_tensors = torch.cat(processed_batches, dim=0)\n",
    "\n",
    "# Store processed tensors back into the subset_reviews dataset\n",
    "for i, review in enumerate(subset_reviews):\n",
    "    review[\"processed_text\"] = processed_tensors[i].tolist()  # Convert tensors to lists for storage\n",
    "    \n",
    "# Display the first record with processed text\n",
    "print(\"First record after GPU-accelerated tokenization:\")\n",
    "print(subset_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137153b-9ee7-4b9c-bc62-1f246fc4e40d",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2db45be5-7ff7-4752-a135-c81bc642c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chetn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Convert stopwords to token IDs\n",
    "stopword_ids = tokenizer.convert_tokens_to_ids(list(stop_words))\n",
    "\n",
    "# Remove `None` values for words not in the tokenizer's vocabulary\n",
    "stopword_ids = [token_id for token_id in stopword_ids if token_id is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f2bf8f5-6942-4560-834a-455bf9bce1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Function to remove stopwords and pad sequences\n",
    "def remove_stopwords_and_pad(tokenized_batch, max_length=128):\n",
    "    token_ids = tokenized_batch[\"input_ids\"]\n",
    "    \n",
    "    # Remove stopwords from each sequence\n",
    "    filtered_tokens = [\n",
    "        [token for token in seq if token not in stopword_ids] for seq in token_ids\n",
    "    ]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    filtered_tensors = [torch.tensor(seq) for seq in filtered_tokens]\n",
    "    \n",
    "    # Pad sequences to max_length\n",
    "    padded_tensors = pad_sequence(filtered_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Truncate sequences to the max_length\n",
    "    return padded_tensors[:, :max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "609a9698-30ed-4a38-9e12-2dca8cf37978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First record after stopword removal and padding:\n",
      "{'text': 'Hibachi Station was delicious and hot and fresh clean restaurant.. stacked bar... Clean restroom', 'stars': 3.0, 'cool': 0, 'funny': 0, 'useful': 0, 'sentiment': 'Neutral', 'processed_text': [101, 7632, 7693, 2072, 2276, 2001, 12090, 1998, 2980, 1998, 4840, 4550, 4825, 1012, 1012, 16934, 3347, 1012, 1012, 1012, 4550, 28249, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'processed_text_no_stopwords': [101, 7632, 7693, 2072, 2276, 12090, 2980, 4840, 4550, 4825, 1012, 1012, 16934, 3347, 1012, 1012, 1012, 4550, 28249, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Apply stopword removal and padding to all batches\n",
    "processed_batches_no_stopwords = [\n",
    "    remove_stopwords_and_pad({\"input_ids\": batch.tolist()}, max_length=128).to(\"cuda\")\n",
    "    for batch in processed_batches\n",
    "]\n",
    "\n",
    "# Flatten the processed batches without stopwords into a single tensor\n",
    "processed_tensors_no_stopwords = torch.cat(processed_batches_no_stopwords, dim=0)\n",
    "\n",
    "# Update the subset_reviews dataset with stopword-removed tokens\n",
    "for i, review in enumerate(subset_reviews):\n",
    "    review[\"processed_text_no_stopwords\"] = processed_tensors_no_stopwords[i].tolist()\n",
    "\n",
    "# Display the first record with stopwords removed\n",
    "print(\"First record after stopword removal and padding:\")\n",
    "print(subset_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a26caf-65e0-4487-8602-1f6aceda5e51",
   "metadata": {},
   "source": [
    "## Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bb0edb8-21ab-4c9f-a120-e2e2ef10d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment mapping\n",
    "sentiment_mapping = {\"Positive\": 2, \"Neutral\": 1, \"Negative\": 0}\n",
    "\n",
    "# Convert labels\n",
    "for review in subset_reviews:\n",
    "    review[\"label\"] = sentiment_mapping[review[\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9ad2032-eaae-405d-b168-279ac1cbb7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as labeled_reviews.json\n"
     ]
    }
   ],
   "source": [
    "# Save the labeled dataset as JSON\n",
    "with open(\"labeled_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(subset_reviews, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Dataset saved as labeled_reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23bdd533-2ddf-4d1a-98cc-2b93b6adea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in subset_reviews: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Display the total number of records in subset_reviews\n",
    "print(f\"Total number of records in subset_reviews: {len(subset_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca3665-9d51-4a65-bedc-0c52e7137827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
